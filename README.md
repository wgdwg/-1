# 机器学习之回归模型-梯度下降法求解线性回归
对损失函数中的参数求偏导得到梯度，带到梯度下降公式中，使得梯度不断下降，即沿着负梯度方向，使得损失函数越来越小，即误差最小，模型越来越好。
线性回归求解可以使用最小二乘法和梯度下降法，下面我们针对两种方法进行对比：
1-相同点：本质和目标相同，两种都是经典的学习算法，在给定已知数据的情况下，利用求导算出一个模型(函数)，使得损失函数最小，然后对给定的新数据进行估算预测。
2-不同点：损失函数的选择：最小二乘法必须使用平方损失函数，而梯度下降可以选取其它函数；实现方法不同，最小二乘法是实现全局最小，而梯度下降是一种迭代法；一般最小二乘法一定可以得到全局最小，但对于多元计算繁琐，且复杂情况下未必有解，而梯度下降的迭代比较简单，但找到的一般是局部最小，即极小值，仅在目标函数是下凸函数是才是全局最小，到最小点的收敛速度会变慢，且对初始点的选择极为敏感，而且步长的选择对损失函数也有影响。
